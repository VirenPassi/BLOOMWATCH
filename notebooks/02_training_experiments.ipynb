{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490b03b3",
   "metadata": {},
   "source": [
    "# BloomWatch: Model Training Experiments\n",
    "\n",
    "Training and experimentation notebook for plant bloom detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from data import PlantBloomDataset, create_standard_transforms\n",
    "from models import SimpleCNN, ResNetBaseline, ModelUtils\n",
    "from models.losses import FocalLoss\n",
    "from utils import ConfigManager, MetricsTracker, get_device, set_seed\n",
    "from visualization import plot_training_metrics\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c84997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / \"configs\" / \"config.yaml\"\n",
    "config_manager = ConfigManager(str(config_path))\n",
    "config = config_manager.config\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"Epochs: {config.training.epochs}\")\n",
    "print(f\"Batch size: {config.data.batch_size}\")\n",
    "print(f\"Learning rate: {config.training.learning_rate}\")\n",
    "print(f\"Model: {config.model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "data_dir = project_root / \"data\" / \"raw\"\n",
    "annotations_file = project_root / \"data\" / \"annotations.csv\"\n",
    "\n",
    "# Create transforms\n",
    "train_transform = create_standard_transforms(\n",
    "    image_size=tuple(config.data.image_size),\n",
    "    is_training=True\n",
    ")\n",
    "val_transform = create_standard_transforms(\n",
    "    image_size=tuple(config.data.image_size),\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    train_dataset = PlantBloomDataset(\n",
    "        data_dir=str(data_dir),\n",
    "        annotations_file=str(annotations_file),\n",
    "        transform=train_transform,\n",
    "        stage='train'\n",
    "    )\n",
    "    \n",
    "    val_dataset = PlantBloomDataset(\n",
    "        data_dir=str(data_dir),\n",
    "        annotations_file=str(annotations_file),\n",
    "        transform=val_transform,\n",
    "        stage='val'\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Using dummy datasets: {e}\")\n",
    "    # Create dummy datasets for demonstration\n",
    "    train_dataset = None\n",
    "    val_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc40d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "if train_dataset and val_dataset:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.data.num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.data.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.data.num_workers\n",
    "    )\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "else:\n",
    "    print(\"Skipping data loader creation (no real data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "if config.model.name == 'simple_cnn':\n",
    "    model = SimpleCNN(\n",
    "        num_classes=config.model.num_classes,\n",
    "        dropout=config.model.dropout\n",
    "    )\n",
    "elif config.model.name == 'resnet':\n",
    "    model = ResNetBaseline(\n",
    "        num_classes=config.model.num_classes,\n",
    "        backbone=config.model.backbone,\n",
    "        pretrained=config.model.pretrained,\n",
    "        dropout=config.model.dropout\n",
    "    )\n",
    "else:\n",
    "    model = SimpleCNN(num_classes=config.model.num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "\n",
    "# Count parameters\n",
    "param_count = ModelUtils.count_parameters(model)\n",
    "print(f\"Parameters: {param_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training components\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.training.learning_rate,\n",
    "    weight_decay=config.training.weight_decay\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=config.training.step_size,\n",
    "    gamma=config.training.gamma\n",
    ")\n",
    "\n",
    "# Metrics tracker\n",
    "metrics_tracker = MetricsTracker(\n",
    "    num_classes=config.model.num_classes,\n",
    "    class_names=['bud', 'early_bloom', 'full_bloom', 'late_bloom', 'dormant']\n",
    ")\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target, _) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target, _ in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50698408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if train_dataset and val_dataset:\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(config.training.epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        metrics = {\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        metrics_tracker.update_epoch_metrics(epoch, metrics)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "else:\n",
    "    print(\"Simulating training with dummy data...\")\n",
    "    # Simulate training metrics for demonstration\n",
    "    for epoch in range(10):\n",
    "        # Simulate improving metrics\n",
    "        train_loss = 2.0 - epoch * 0.15 + np.random.normal(0, 0.1)\n",
    "        val_loss = 2.2 - epoch * 0.12 + np.random.normal(0, 0.1)\n",
    "        train_acc = 0.2 + epoch * 0.08 + np.random.normal(0, 0.02)\n",
    "        val_acc = 0.15 + epoch * 0.07 + np.random.normal(0, 0.02)\n",
    "        \n",
    "        metrics = {\n",
    "            'train_loss': max(0.1, train_loss),\n",
    "            'val_loss': max(0.1, val_loss),\n",
    "            'train_acc': min(0.95, max(0.1, train_acc)),\n",
    "            'val_acc': min(0.9, max(0.1, val_acc)),\n",
    "            'learning_rate': 0.001 * (0.9 ** (epoch // 3))\n",
    "        }\n",
    "        metrics_tracker.update_epoch_metrics(epoch, metrics)\n",
    "    \n",
    "    print(\"Simulation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "fig = metrics_tracker.plot_metrics_history()\n",
    "plt.show()\n",
    "\n",
    "# Print best metrics\n",
    "best_metrics = metrics_tracker.best_metrics\n",
    "print(\"\\nBest Metrics:\")\n",
    "for metric, value in best_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eea1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and metrics\n",
    "save_dir = project_root / \"checkpoints\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = save_dir / \"best_model.pth\"\n",
    "if train_dataset:  # Only save if we actually trained\n",
    "    ModelUtils.save_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=best_metrics['best_epoch'],\n",
    "        loss=best_metrics['best_val_loss'],\n",
    "        filepath=str(model_path),\n",
    "        metadata={'config': config, 'metrics': best_metrics}\n",
    "    )\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = save_dir / \"training_metrics.json\"\n",
    "metrics_tracker.save_metrics(str(metrics_path))\n",
    "print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "print(\"\\nTraining experiment completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
